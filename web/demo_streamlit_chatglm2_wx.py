import json
import torch
import streamlit as st # 1.26.0
from transformers.generation.utils import GenerationConfig
from peft import PeftModel
import sys


sys.path.append("/")
from models.chatglm2.modeling_chatglm import ChatGLMForConditionalGeneration
from models.chatglm2.tokenization_chatglm import ChatGLMTokenizer
st.set_page_config(page_title="ChatGLM2-6B-Chat-WX")
st.title("ChatGLM2-6B-Chat-WX")


@st.cache_resource
def init_model():
    model_path = "/data/pretrained_models/chatglm2-6b-20230625"
    peft_model_path = "/data1/wufan/experiments/llm/wx_bilu_v2.1.0/chatglm-6b-lora-wx-1e-5/checkpoint-1000"

    model = ChatGLMForConditionalGeneration.from_pretrained(
        model_path,
        torch_dtype=torch.float16,
        device_map="auto",
    )
    model = PeftModel.from_pretrained(model, peft_model_path, device_map='auto')

    tokenizer = ChatGLMTokenizer.from_pretrained(
        model_path,
        use_fast=False,
        trust_remote_code=True
    )
    return model, tokenizer

def generate_prompt(input):
    instruction = """###Instruction:

æ ¹æ®å¥å­å†…å®¹ï¼Œé’ˆå¯¹å¥å­ä¸­æœªæé—®çš„é—®é¢˜æˆ–è€…å·²ç»æåˆ°çš„äº‹æƒ…è¿›ä¸€æ­¥æé—®ï¼Œè¿”å›å‡ ä¸ªçš„æé—®çš„ç»“æœï¼Œå¹¶æ»¡è¶³å¦‚ä¸‹å‡ ç‚¹è¦æ±‚ï¼š
1.å¦‚æœåœ¨æé—®å¹¶å›ç­”å¦‚ä¸‹è¯é¢˜ï¼šä¸ªäººæƒ…å†µï¼Œä¸ªäººç®€å†ï¼Œå®¶åº­æˆå‘˜ï¼Œæ³•å¾‹æ¡æ¬¾ï¼Œèº«ä½“çŠ¶å†µç­‰ï¼Œè¿”å›çš„é—®é¢˜å¯ä»¥å‚è€ƒä½†ä¸å±€é™äºï¼šå› ä¸ºä»€ä¹ˆäº‹æƒ…æŠ¥æ¡ˆï¼Ÿæè¿°ä¸€ä¸‹å…·ä½“äº‹æƒ…å‘ç”Ÿçš„ç»è¿‡ï¼Ÿ
2.å¦‚æœåœ¨æé—®å¹¶å›ç­”æ¡ˆä»¶ç»è¿‡ï¼Œéœ€è¦ä¾æ®äººç‰©ï¼Œæ—¶é—´ï¼Œåœ°ç‚¹ï¼Œäº‹ä»¶å†…å®¹ï¼Œè¡¥å……å¥å­ä¸­æœªæåŠçš„é—®é¢˜ï¼›
3.å¦‚æœäº‹å‘ç»è¿‡ä¸­ï¼ŒæœªæåŠäº‹æƒ…å‘ç”Ÿçš„æ—¶é—´ã€åœ°ç‚¹ï¼Œè¯·è¡¥å……æé—®ï¼›
4.ä¸èƒ½æé—®ä¸å¥å­æ— å…³çš„å†…å®¹ï¼›
5.ä¸éœ€è¦å›ç­”å¥å­ä¸­çš„é—®é¢˜ï¼›
6.é—®é¢˜åœ¨å¯¹è¯ä¸­ä¸èƒ½æœ‰ç­”æ¡ˆï¼›
7.é—®é¢˜éœ€è¦å¯¹è­¦å¯Ÿæ¢³ç†æ¡ˆä»¶æœ‰æ­£å‘ä¿ƒè¿›ä½œç”¨ï¼›
8.ä¸èƒ½æé—®å¥å­ä¸­å·²ç»å­˜åœ¨æˆ–è€…ç›¸ä¼¼çš„é—®é¢˜ï¼›
9.æé—®5~10ä¸ªé—®é¢˜ï¼Œæ¯ä¸ªé—®é¢˜ä¸å°‘äº15å­—

"""
    return f"""[Round 0]\né—®:{instruction}###Input:\n{input}\nç­”ï¼š"""

def clear_chat_history():
    del st.session_state.messages


def init_chat_history():
    with st.chat_message("assistant", avatar='ğŸ¤–'):
        st.markdown("æ‚¨å¥½ï¼Œæˆ‘æ˜¯ChatGLM2å¾®è°ƒçš„è®¯é—®æŒ‡å¼•å¤§æ¨¡å‹ï¼Œå¾ˆé«˜å…´ä¸ºæ‚¨æœåŠ¡ğŸ¥°")

    if "messages" in st.session_state:
        for message in st.session_state.messages:
            avatar = 'ğŸ§‘â€ğŸ’»' if message["role"] == "user" else 'ğŸ¤–'
            with st.chat_message(message["role"], avatar=avatar):
                st.markdown(message["content"])
    else:
        st.session_state.messages = []

    return st.session_state.messages


def main():

    model, tokenizer = init_model()
    messages = init_chat_history()
    # TODO: éœ€è¦python3.8
    if prompt := st.chat_input("Shift + Enter æ¢è¡Œ, Enter å‘é€"):
        with st.chat_message("user", avatar='ğŸ§‘â€ğŸ’»'):
            st.markdown(prompt)
        # messages.append({"role": "user", "content": prompt})
        # print(json.dumps({"role": "user", "content": prompt},ensure_ascii=False),flush=True)
        print(f"[user]:\n {prompt}", flush=True)
        with st.chat_message("assistant", avatar='ğŸ¤–'):
            placeholder = st.empty()
            instruction = generate_prompt(prompt)
            for response, history in model.stream_chat(tokenizer, instruction, history = []):
                placeholder.markdown(response)
                if torch.backends.mps.is_available():
                    torch.mps.empty_cache()
        # messages.append({"role": "assistant", "content": response})
        print(f"[assistant]:\n {response}\n\n", flush=True)

        st.button("æ¸…ç©ºå¯¹è¯", on_click=clear_chat_history)


if __name__ == "__main__":
    main()
