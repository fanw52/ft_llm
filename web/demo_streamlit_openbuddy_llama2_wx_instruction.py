import json

import streamlit as st  # 1.26.0
import torch
from peft import PeftModel
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation.utils import GenerationConfig

st.set_page_config(page_title="OpenBuddy-13B-Chat-WX")
st.title("OpenBuddy-13B-Chat-WX")


@st.cache_resource
def init_model():
    model_path = "/data/pretrained_models/openbuddy-llama2-13b-v8.1-fp16"

    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True
    )

    model.generation_config = GenerationConfig.from_pretrained(
        model_path
    )
    tokenizer = AutoTokenizer.from_pretrained(
        model_path,
        use_fast=False,
        trust_remote_code=True
    )
    return model, tokenizer

def generate_prompt(input):
    instruction = """###Instruction:

æ ¹æ®å¥å­å†…å®¹ï¼Œé’ˆå¯¹å¥å­ä¸­æœªæé—®çš„é—®é¢˜æˆ–è€…å·²ç»æåˆ°çš„äº‹æƒ…è¿›ä¸€æ­¥æé—®ï¼Œè¿”å›å‡ ä¸ªçš„æé—®çš„ç»“æœï¼Œå¹¶æ»¡è¶³å¦‚ä¸‹å‡ ç‚¹è¦æ±‚ï¼š
1.å¦‚æœåœ¨æé—®å¹¶å›ç­”å¦‚ä¸‹è¯é¢˜ï¼šä¸ªäººæƒ…å†µï¼Œä¸ªäººç®€å†ï¼Œå®¶åº­æˆå‘˜ï¼Œæ³•å¾‹æ¡æ¬¾ï¼Œèº«ä½“çŠ¶å†µç­‰ï¼Œè¿”å›çš„é—®é¢˜å¯ä»¥å‚è€ƒä½†ä¸å±€é™äºï¼šå› ä¸ºä»€ä¹ˆäº‹æƒ…æŠ¥æ¡ˆï¼Ÿæè¿°ä¸€ä¸‹å…·ä½“äº‹æƒ…å‘ç”Ÿçš„ç»è¿‡ï¼Ÿ
2.å¦‚æœåœ¨æé—®å¹¶å›ç­”æ¡ˆä»¶ç»è¿‡ï¼Œéœ€è¦ä¾æ®äººç‰©ï¼Œæ—¶é—´ï¼Œåœ°ç‚¹ï¼Œäº‹ä»¶å†…å®¹ï¼Œè¡¥å……å¥å­ä¸­æœªæåŠçš„é—®é¢˜ï¼›
3.å¦‚æœäº‹å‘ç»è¿‡ä¸­ï¼ŒæœªæåŠäº‹æƒ…å‘ç”Ÿçš„æ—¶é—´ã€åœ°ç‚¹ï¼Œè¯·è¡¥å……æé—®ï¼›
4.ä¸èƒ½æé—®ä¸å¥å­æ— å…³çš„å†…å®¹ï¼›
5.ä¸éœ€è¦å›ç­”å¥å­ä¸­çš„é—®é¢˜ï¼›
6.é—®é¢˜åœ¨å¯¹è¯ä¸­ä¸èƒ½æœ‰ç­”æ¡ˆï¼›
7.é—®é¢˜éœ€è¦å¯¹è­¦å¯Ÿæ¢³ç†æ¡ˆä»¶æœ‰æ­£å‘ä¿ƒè¿›ä½œç”¨ï¼›
8.ä¸èƒ½æé—®å¥å­ä¸­å·²ç»å­˜åœ¨æˆ–è€…ç›¸ä¼¼çš„é—®é¢˜ï¼›
9.æé—®5~10ä¸ªé—®é¢˜ï¼Œæ¯ä¸ªé—®é¢˜ä¸å°‘äº15å­—ï¼›
10.å¦‚æœè¾“å…¥çš„å†…å®¹æ— æ³•ç†è§£ï¼Œè¯·å›ç­”ï¼šæ— æ³•ç†è§£è¾“å…¥çš„å†…å®¹ï¼Œè¯·é‡æ–°ç»„ç»‡è¯­è¨€

"""
    return f"""\nUser:{instruction}###Input:\n{input}\n\nAssistant:"""

def sample_top_p(probs, p):
    # probs: [bs, 1, vocab_size]

    probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)

    probs_sum = torch.cumsum(probs_sort, dim=-1)
    mask = probs_sum - probs_sort > p
    probs_sort[mask] = 0.0

    probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))
    next_token = torch.multinomial(probs_sort, num_samples=1)
    next_token = torch.gather(probs_idx, -1, next_token)

    return next_token


def stream_generate(model, tokenizer, instruction, max_length, top_p, temperature):
    """ æµå¼ç”Ÿæˆæ–‡æœ¬ï¼Œé¡µé¢äº¤äº’ä½“éªŒæ›´å‹å¥½ä¸€äº› """

    with torch.no_grad():
        batch = tokenizer(instruction, return_tensors="pt")
        in_tokens = batch['input_ids'].cuda()
        prompt_len = len(in_tokens[0])

        if len(in_tokens[0]) > max_length - 128:
            return f"è¾“å…¥tokenæ€»é•¿åº¦è¶…è¿‡äº† max_len-128ï¼Œè¾“å…¥tokené•¿åº¦ï¼š{len(in_tokens[0])}ï¼Œmax_lengthï¼š{max_length}ã€‚è¯·æ¸…ç©ºå†å²æ•°æ®å¼€å§‹æ–°çš„å¯¹è¯ã€‚", ""

        end_token = torch.tensor([tokenizer.eos_token_id]).to(in_tokens.device)
        past_key_values = None
        out_tokens = None

        # ç”Ÿæˆçš„æ–‡æœ¬è¾¾åˆ°æœ€å¤§é•¿åº¦æ—¶åœæ­¢æ¨ç†ã€é‡åˆ°ç»ˆæ­¢å­—ç¬¦æ—¶åœæ­¢æ¨ç†
        pre_text = ""
        while (out_tokens is None) or (
                (out_tokens[0][-1] != end_token) and (prompt_len + out_tokens.size()[1] < max_length)):
            forward_result = model(input_ids=in_tokens, past_key_values=past_key_values, use_cache=True)
            logits = forward_result.logits
            past_key_values = forward_result.past_key_values

            if temperature > 0:
                probs = torch.softmax(logits[:, -1, :] / temperature, dim=-1)
                out_token = sample_top_p(probs, top_p)
            else:
                out_token = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)

            in_tokens = out_token

            if out_tokens is None:
                out_tokens = out_token
            else:
                out_tokens = torch.cat([out_tokens, out_token], dim=-1)

            total_text = tokenizer.decode(out_tokens[0])

            new_token = total_text[len(pre_text):]
            pre_text = total_text
            yield new_token


def clear_chat_history():
    del st.session_state.messages


def init_chat_history():
    with st.chat_message("assistant", avatar='ğŸ¤–'):
        st.markdown("æ‚¨å¥½ï¼Œæˆ‘æ˜¯OpenBuddyè®¯é—®æŒ‡å¼•å¤§æ¨¡å‹(æŒ‡ä»¤ç‰ˆ)ï¼Œå¾ˆé«˜å…´ä¸ºæ‚¨æœåŠ¡ğŸ¥°")

    if "messages" in st.session_state:
        for message in st.session_state.messages:
            avatar = 'ğŸ§‘â€ğŸ’»' if message["role"] == "user" else 'ğŸ¤–'
            with st.chat_message(message["role"], avatar=avatar):
                st.markdown(message["content"])
    else:
        st.session_state.messages = []

    return st.session_state.messages


def main():
    model, tokenizer = init_model()
    messages = init_chat_history()
    # TODO: éœ€è¦python3.8
    if prompt := st.chat_input("Shift + Enter æ¢è¡Œ, Enter å‘é€"):
        with st.chat_message("user", avatar='ğŸ§‘â€ğŸ’»'):
            st.markdown(prompt)
        messages.append({"role": "user", "content": prompt})
        print(f"[user]:\n {prompt}", flush=True)
        #TODO: æ¯æ¬¡ä¸ä¼šä¼ å…¥å†å²å¯¹è¯ä¿¡æ¯
        with st.chat_message("assistant", avatar='ğŸ¤–'):
            placeholder = st.empty()
            instruction = generate_prompt(prompt)
            pre_response = ""
            for response in stream_generate(model,tokenizer,instruction,max_length=2048,top_p=0.9, temperature=1):
                pre_response += response
                if "</s>" in pre_response:
                    pre_response = pre_response[:-4]
                placeholder.markdown(pre_response)
                if torch.backends.mps.is_available():
                    torch.mps.empty_cache()
        pre_response.replace("</s>", "")
        print(pre_response)
        messages.append({"role": "assistant", "content": pre_response})
        # print(json.dumps(messages, ensure_ascii=False), flush=True)
        print(f"[assistant]:\n {response}\n\n", flush=True)

        st.button("æ¸…ç©ºå¯¹è¯", on_click=clear_chat_history)


if __name__ == "__main__":
    main()
